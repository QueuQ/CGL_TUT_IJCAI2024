---
layout: default
---

## Materials

Slides for the tutorial: <a href="https://github.com/QueuQ/CGL_TUT_IJCAI2024/blob/main/Tutorial_CGL_IJCAI2024.pdf"> https://github.com/QueuQ/CGL_TUT_IJCAI2024/blob/main/Tutorial_CGL_IJCAI2024.pdf </a>

Our survey on continual graph learning: <a href="https://arxiv.org/abs/2402.11565"> https://arxiv.org/abs/2402.11565 </a>

We also maintain an up-to-date GitHub repository featuring a comprehensive list of CGL algorithms, accessible at
<a href="https://github.com/UConn-DSIS/Survey-of-Continual-Learning-on-Graphs"> https://github.com/UConn-DSIS/Survey-of-Continual-Learning-on-Graphs.

Other recommended works include:
1. CGLB: Benchmark tasks for continual graph learning [link](https://openreview.net/forum?id=5wNiiIDynDF)
2. Neural message passing for quantum chemistry [link](https://proceedings.mlr.press/v70/gilmer17a)
3. Continual lifelong learning with neural networks: A review [link](https://www.sciencedirect.com/science/article/pii/S0893608019300231)
4. Three scenarios for continual learning [link](https://arxiv.org/abs/1904.07734)


## Tutorial Outline
Most real-world graphs constantly grow or evolve with potential distribution shifts. Classical graph learning models, however, typically assume graphs to be static and suffer from catastrophic forgetting when new types of nodes and edges (or graphs) continuously emerge.  Therefore, investigating how to constantly adapt a graph learning model to new distributions/tasks in the growing graphs without forgetting the previously learned knowledge, i.e. Continual Graph Learning (CGL), is becoming increasingly important in various real-world applications, e.g., social science, biomedical research, etc. Due to the existence of complex topological structures, CGL is essentially different from traditional continual learning on independent data without topological connections (e.g., images). Challenges in CGL include the task configuration in different types of graphs, preservation of the previously learned topology, properly handling the concept drift caused by the topological connections, etc. In this tutorial, we will introduce this newly emerging area - Continual Graph Learning (CGL). Specifically, we will (1) introduce different continual graph learning settings based on various application scenarios, (2) present the key challenges in CGL, (3) highlight the existing CGL techniques and benchmarks, and (4) discuss potential future directions.

## Goal of the tutorial

Continual Graph Learning (CGL) investigates the graph learning problem in a highly practical scenario where the graphs are constantly growing or evolving, which is under-explored and is relevant to various artificial intelligence research directions, as well as tremendous real-world application scenarios (e.g., social science, biomedical research, etc). Therefore, our tutorial will not only attract interest from the audience with artificial intelligence, machine learning, and data mining background but also will attract interest from researchers focusing on specific applications, e.g., social networks, recommender systems, Internet of Things (IoT) networks, drug discovery, etc. 

In this tutorial, we will introduce this newly emerging area - Continual Graph Learning (CGL). Specifically, we will (1) introduce different CGL settings based on various application scenarios, (2) present the key challenges in CGL, (3) highlight the existing CGL techniques and benchmarks, and (4) discuss potential future directions, as well as the relationship between CGL and various other research directions.


## Tutors

* Xikun Zhang is Ph.D. student at the School of Computer Science in The University of Sydney. His research interests mainly include applying deep learning to tackle graph related problems, which is closely related to the topic of this tutorial. His works have been published in top conferences and journals, such as NeurIPS, TPAMI, ICDM, CVPR, ECCV, and TNNLS. 
Email: xzha0505@uni.sydney.edu.au


* Dongjin Song is an assistant professor in the Department of Computer Science and Engineering at the University of Connecticut (UConn). His research interests include machine learning, deep learning, data mining, and related applications for time series data and graph representation learning. Papers describing his research have been published at top-tier data science and artificial intelligence conferences, such as NeurIPS, ICML, ICLR, KDD, ICDM, SDM,  AAAI, IJCAI, CVPR, ICCV, etc. He has served as PC chairs for AI for Time Series (AI4TS) workshop at IJCAI 2022 and the Mining and Learning from Time Series workshop at KDD 2022. He has also served as senior PC members for AAAI, IJCAI, and CIKM. He won the UConn Research Excellence Research (REP) Award in 2021.
Email: dongjin.song@uconn.edu

  
* Dacheng Tao (Fellow, IEEE) is currently a Distinguished University Professor in the College of Computing & Data Science at Nanyang Technological University. He mainly applies statistics and mathematics to artificial intelligence and data science. His research is detailed in one monograph and over 200 publications in prestigious journals and proceedings at prominent conferences such as IEEE TPAMI, IJCV, JMLR, NeurIPS, ICML, ICLR, CVPR, ICCV, ECCV, AAAI, IJCAI, ICDM and ACM SIGKDD, with several best paper awards, such as the Best Theory/Algorithm Paper Runner Up Award at IEEE ICDMâ€™07, the Distinguished Paper Award at 2018 IJCAI, the 2014 ICDM 10-year Highest-Impact Paper Award, and the 2017 IEEE Signal Processing Society Best Paper Award. He received the 2015 and 2020 Australian Eureka Prize and the 2018 IEEE ICDM Research Contributions Award. He is a fellow of the Australian Academy of Science, TWAS, AAAS, ACM and IEEE.
Email: dacheng.tao@ntu.edu.sg
